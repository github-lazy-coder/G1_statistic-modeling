library(readxl)     # read excel file
library(tidyverse) 
library(corrplot)
library(glmnet)     # LASSO
library(leaps)      # LASSO
library(ggplot2)
library(dplyr)

# read data
setwd("D:/文档/HSUHK/AMS6002 Statistical Modeling/group project")
rb_Data <- read_xlsx('Residential-Building-Data-Set.xlsx', skip = 1)
rb_Data <- data.frame(rb_Data)

# data clean
rb_Data <- rb_Data %>% drop_na() %>% unique()
summary(rb_Data)

# LASSO for sales

# get V.2-V.8
base_data <- rb_Data[,6:12]
# get sales
base_data <- cbind(base_data, rb_Data[,108])
colnames(base_data)[8] <- c("V.9")
# Split it into 5 tables.
lag1_data <- rb_Data[,13:31]
lag2_data <- rb_Data[,32:50]
lag3_data <- rb_Data[,51:69]
lag4_data <- rb_Data[,70:88]
lag5_data <- rb_Data[,89:107]
# rename columns name
colnames(lag1_data) <- c("V.11", "V.12", "V.13", "V.14", "V.15", "V.16", "V.17", "V.18", "V.19", "V.20", "V.21", "V.22", "V.23", "V.24", "V.25", "V.26", "V.27", "V.28", "V.29")
colnames(lag2_data) <- c("V.11", "V.12", "V.13", "V.14", "V.15", "V.16", "V.17", "V.18", "V.19", "V.20", "V.21", "V.22", "V.23", "V.24", "V.25", "V.26", "V.27", "V.28", "V.29")
colnames(lag3_data) <- c("V.11", "V.12", "V.13", "V.14", "V.15", "V.16", "V.17", "V.18", "V.19", "V.20", "V.21", "V.22", "V.23", "V.24", "V.25", "V.26", "V.27", "V.28", "V.29")
colnames(lag4_data) <- c("V.11", "V.12", "V.13", "V.14", "V.15", "V.16", "V.17", "V.18", "V.19", "V.20", "V.21", "V.22", "V.23", "V.24", "V.25", "V.26", "V.27", "V.28", "V.29")
colnames(lag5_data) <- c("V.11", "V.12", "V.13", "V.14", "V.15", "V.16", "V.17", "V.18", "V.19", "V.20", "V.21", "V.22", "V.23", "V.24", "V.25", "V.26", "V.27", "V.28", "V.29")
# get five dataset
lag1_data <- cbind(base_data, lag1_data)
lag2_data <- cbind(base_data, lag2_data)
lag3_data <- cbind(base_data, lag3_data)
lag4_data <- cbind(base_data, lag4_data)
lag5_data <- cbind(base_data, lag5_data)

# lag1
# Split the lag1 into training and testing datasets.
set.seed(12345)
train_indices = sample(1:nrow(lag1_data),0.7*nrow(lag1_data),replace=F)
train_data = lag1_data[train_indices,]
test_data = lag1_data[-train_indices,]

# Use cv.glmnet to do LASSO automatically
x=model.matrix(V.9~ ., train_data)[,-1]       # Get the design matrix. Dummy variables for categorical data are added. The column of ones (corresponding to the intercept term) is removed.
y=train_data$V.9

# Standardizing
xs = scale(x)

cv.out = cv.glmnet(xs,y,alpha=1,nfolds = 10)
bestlam = cv.out$lambda.min
coef = predict(cv.out ,type="coefficients",s=bestlam)
coef

# after Standardizing nothing change
# after LASSO drop V.20,V.21,V.22,V.25 columns
LASSO_sales_model_1 = lm(V.9 ~ V.2+V.3+V.4+V.5+V.6+V.7+V.8+V.11+V.12+V.13+V.14+V.15+V.16+V.17+V.18+V.19+V.23+V.24+V.26+V.27+V.28+V.29, train_data)

predict_sales_LASSO_1 = predict(LASSO_sales_model_1, test_data)
summary(predict_sales_LASSO_1)

rmse_sales_1 = sqrt(mean((test_data$V.9 - predict_sales_LASSO_1)^2))
rmse_sales_1
# 174.5394

# lag2
# Split the lag2 into training and testing datasets.
set.seed(12345)
train_indices = sample(1:nrow(lag2_data),0.7*nrow(lag2_data),replace=F)
train_data = lag2_data[train_indices,]
test_data = lag2_data[-train_indices,]

# Use cv.glmnet to do LASSO automatically
x=model.matrix(V.9~ ., train_data)[,-1]       # Get the design matrix. Dummy variables for categorical data are added. The column of ones (corresponding to the intercept term) is removed.
y=train_data$V.9

# Standardizing
xs = scale(x)

cv.out = cv.glmnet(xs,y,alpha=1,nfolds = 10)
bestlam = cv.out$lambda.min
coef = predict(cv.out ,type="coefficients",s=bestlam)
coef
# after Standardizing nothing change
# after LASSO drop V.11 column
LASSO_sales_model_2 = lm(V.9 ~ V.2+V.3+V.4+V.5+V.6+V.7+V.8+V.12+V.13+V.14+V.15+V.16+V.17+V.18+V.19+V.20+V.21+V.22+V.23+V.24+V.25+V.26+V.27+V.28+V.29, train_data)

predict_sales_LASSO_2 = predict(LASSO_sales_model_2, test_data)
summary(predict_sales_LASSO_2)

rmse_sales_2 = sqrt(mean((test_data$V.9 - predict_sales_LASSO_2)^2))
rmse_sales_2
# 168.7033

# lag3
# Split the lag3 into training and testing datasets.
set.seed(12345)
train_indices = sample(1:nrow(lag3_data),0.7*nrow(lag3_data),replace=F)
train_data = lag3_data[train_indices,]
test_data = lag3_data[-train_indices,]

# Use cv.glmnet to do LASSO automatically
x=model.matrix(V.9~ ., train_data)[,-1]       # Get the design matrix. Dummy variables for categorical data are added. The column of ones (corresponding to the intercept term) is removed.
y=train_data$V.9

# Standardizing
xs = scale(x)

cv.out = cv.glmnet(xs,y,alpha=1,nfolds = 10)
bestlam = cv.out$lambda.min
coef = predict(cv.out ,type="coefficients",s=bestlam)
coef

# after Standardizing nothing change
# after LASSO drop V.3,V.4,V.12,V.13,V.14,V.21,V.22,V.25,V.26,V.27 columns
LASSO_sales_model_3 = lm(V.9 ~ V.2+V.5+V.6+V.7+V.8+V.11+V.15+V.16+V.17+V.18+V.19+V.20+V.23+V.24+V.28+V.29, train_data)

predict_sales_LASSO_3 = predict(LASSO_sales_model_3, test_data)
summary(predict_sales_LASSO_3)

rmse_sales_3 = sqrt(mean((test_data$V.9 - predict_sales_LASSO_3)^2))
rmse_sales_3
# 187.2429

# lag4
# Split the lag4 into training and testing datasets.
set.seed(12345)
train_indices = sample(1:nrow(lag4_data),0.7*nrow(lag4_data),replace=F)
train_data = lag4_data[train_indices,]
test_data = lag4_data[-train_indices,]

# Use cv.glmnet to do LASSO automatically
x=model.matrix(V.9~ ., train_data)[,-1]       # Get the design matrix. Dummy variables for categorical data are added. The column of ones (corresponding to the intercept term) is removed.
y=train_data$V.9

# Standardizing
xs = scale(x)

cv.out = cv.glmnet(xs,y,alpha=1,nfolds = 10)
bestlam = cv.out$lambda.min
coef = predict(cv.out ,type="coefficients",s=bestlam)
coef

# before Standardizing
# after LASSO drop V.4,V12,V.13,V.16,V.18,V.22 column
# LASSO_sales_model_4 = lm(V.9 ~ V.2+V.3+V.5+V.6+V.7+V.8+V.11+V.14+V.15+V.17+V.19+V.20+V.21+V.23+V.24+V.25+V.26+V.27+V.28+V.29, train_data)
# after Standardizing
# after LASSO drop V.4,V12,V.16,V.18,V.21,V.22,V.25 column
LASSO_sales_model_4 = lm(V.9 ~ V.2+V.3+V.5+V.6+V.7+V.8+V.11+V.13+V.14+V.15+V.17+V.19+V.20+V.23+V.24+V.26+V.27+V.28+V.29, train_data)

predict_sales_LASSO_4 = predict(LASSO_sales_model_4, test_data)
summary(predict_sales_LASSO_4)

rmse_sales_4 = sqrt(mean((test_data$V.9 - predict_sales_LASSO_4)^2))
rmse_sales_4
# before Standardizing 165.7134
# after Standardizing 165.0812

# lag5
# Split the lag5 into training and testing datasets.
set.seed(12345)
train_indices = sample(1:nrow(lag5_data),0.7*nrow(lag5_data),replace=F)
train_data = lag5_data[train_indices,]
test_data = lag5_data[-train_indices,]

# Use cv.glmnet to do LASSO automatically
x=model.matrix(V.9~ ., train_data)[,-1]       # Get the design matrix. Dummy variables for categorical data are added. The column of ones (corresponding to the intercept term) is removed.
y=train_data$V.9

# Standardizing
xs = scale(x)

cv.out = cv.glmnet(xs,y,alpha=1,nfolds = 10)
bestlam = cv.out$lambda.min
coef = predict(cv.out ,type="coefficients",s=bestlam)
coef

# before Standardizing
# after LASSO drop V.3,V.12,V.13,V.16,V.19,V.22,V.24,V.25,V.27,V.29 columns
# LASSO_sales_model_5 = lm(V.9 ~ V.2+V.4+V.5+V.6+V.7+V.8+V.11+V.14+V.15+V.17+V.18+V.20+V.21+V.23+V.26+V.28, train_data)
# after Standardizing
# after LASSO drop V.3,V.12,V.13,V.14,V.16,V.19,V.22,V.24,V.25,V.27,V.28,V.29 columns
LASSO_sales_model_5 = lm(V.9 ~ V.2+V.4+V.5+V.6+V.7+V.8+V.11+V.15+V.17+V.18+V.20+V.21+V.23+V.26, train_data)

predict_sales_LASSO_5 = predict(LASSO_sales_model_5, test_data)
summary(predict_sales_LASSO_5)

rmse_sales_5 = sqrt(mean((test_data$V.9 - predict_sales_LASSO_5)^2))
rmse_sales_5
# before Standardizing 165.1048
# after Standardizing 163.2143

# lag_avg
# get lag_avg
lag_avg_1_data <- rb_Data[,13:31]
lag_avg_2_data <- rb_Data[,32:50]
lag_avg_3_data <- rb_Data[,51:69]
lag_avg_4_data <- rb_Data[,70:88]
lag_avg_5_data <- rb_Data[,89:107]
lag_avg <- c((lag_avg_1_data+lag_avg_2_data+lag_avg_3_data+lag_avg_4_data+lag_avg_5_data)/5)
lag_avg <- data.frame(lag_avg)
colnames(lag_avg) <- c("V.11", "V.12", "V.13", "V.14", "V.15", "V.16", "V.17", "V.18", "V.19", "V.20", "V.21", "V.22", "V.23", "V.24", "V.25", "V.26", "V.27", "V.28", "V.29")
lag_avg <- cbind(base_data, lag_avg)

head(lag_avg, n = 5)
summary(lag_avg)

set.seed(12345)
train_indices = sample(1:nrow(lag_avg),0.7*nrow(lag_avg),replace=F)
train_data = lag_avg[train_indices,]
test_data = lag_avg[-train_indices,]

# Use cv.glmnet to do LASSO automatically
x=model.matrix(V.9~ ., train_data)[,-1]                 # Get the design matrix. Dummy variables for categorical data are added. The column of ones (corresponding to the intercept term) is removed.
y=train_data$V.9

# Standardizing
xs = scale(x)

cv.out = cv.glmnet(xs,y,alpha=1,nfolds = 10)
bestlam = cv.out$lambda.min
coef = predict(cv.out ,type="coefficients",s=bestlam)
coef

# after Standardizing nothing change
# after LASSO drop V.3,V.4,V.12,V.13,V.14,V.15,V.21,V.22,V.24,V.25,V.26,V.29 columns
LASSO_sales_avg_model = lm(V.9 ~ V.2+V.5+V.6+V.7+V.8+V.11+V.16+V.17+V.18+V.19+V.20+V.23+V.27+V.28,train_data)

predict_sales_avg_LASSO = predict(LASSO_sales_avg_model, test_data)
summary(predict_sales_avg_LASSO)

rmse_sales_avg = sqrt(mean((test_data$V.9 - predict_sales_avg_LASSO)^2))
rmse_sales_avg
# 168.111

################################################### cost LASSO ###################################################################################

# LASSO for cost

# lag1
# Split the lag1 into training and testing datasets.
# change sales to cost
lag1_data <- cbind(lag1_data[,-8], rb_Data[,109])
# rename columns name
colnames(lag1_data)[27] <- c("V.10")
set.seed(12345)
train_indices = sample(1:nrow(lag1_data),0.7*nrow(lag1_data),replace=F)
train_data = lag1_data[train_indices,]
test_data = lag1_data[-train_indices,]

# Use cv.glmnet to do LASSO automatically
x=model.matrix(V.10~ ., train_data)[,-1]       # Get the design matrix. Dummy variables for categorical data are added. The column of ones (corresponding to the intercept term) is removed.
y=train_data$V.10

# Standardizing
xs = scale(x)

cv.out = cv.glmnet(xs,y,alpha=1,nfolds = 10)
bestlam = cv.out$lambda.min
coef = predict(cv.out ,type="coefficients",s=bestlam)
coef

# before Standardizing
# after LASSO all columns selected
# LASSO_cost_model_1 = lm(V.10 ~ ., train_data)
# after Standardizing
# after LASSO drop V.13,V.22 columns
LASSO_cost_model_1 = lm(V.10 ~ V.2+V.3+V.4+V.5+V.6+V.7+V.8+V.11+V.12+V.14+V.15+V.16+V.17+V.18+V.19+V.20+V.21+V.23+V.24+V.25+V.26+V.27+V.28+V.29, train_data)

predict_cost_LASSO_1 = predict(LASSO_cost_model_1, test_data)
summary(predict_cost_LASSO_1)

rmse_cost_1 = sqrt(mean((test_data$V.10 - predict_cost_LASSO_1)^2))
rmse_cost_1
# before Standardizing 40.18401
# after Standardizing 40.13801

# lag2
# Split the lag2 into training and testing datasets.
# change sales to cost
lag2_data <- cbind(lag2_data[,-8], rb_Data[,109])
# rename columns name
colnames(lag2_data)[27] <- c("V.10")
set.seed(12345)
train_indices = sample(1:nrow(lag2_data),0.7*nrow(lag2_data),replace=F)
train_data = lag2_data[train_indices,]
test_data = lag2_data[-train_indices,]

# Use cv.glmnet to do LASSO automatically
x=model.matrix(V.10~ ., train_data)[,-1]       # Get the design matrix. Dummy variables for categorical data are added. The column of ones (corresponding to the intercept term) is removed.
y=train_data$V.10

# Standardizing
xs = scale(x)

cv.out = cv.glmnet(xs,y,alpha=1,nfolds = 10)
bestlam = cv.out$lambda.min
coef = predict(cv.out ,type="coefficients",s=bestlam)
coef

# before Standardizing
# after LASSO drop V.22,V.25 column
# LASSO_cost_model_2 = lm(V.10 ~ V.2+V.3+V.4+V.5+V.6+V.7+V.8+V.11+V.12+V.13+V.14+V.15+V.16+V.17+V.18+V.19+V.20+V.21+V.23+V.24+V.26+V.27+V.28+V.29, train_data)
# # after Standardizing
# after LASSO drop V.13,V.14,V.22,V.25 column
LASSO_cost_model_2 = lm(V.10 ~ V.2+V.3+V.4+V.5+V.6+V.7+V.8+V.11+V.12+V.15+V.16+V.17+V.18+V.19+V.20+V.21+V.23+V.24+V.26+V.27+V.28+V.29, train_data)

predict_cost_LASSO_2 = predict(LASSO_cost_model_2, test_data)
summary(predict_cost_LASSO_2)

rmse_cost_2 = sqrt(mean((test_data$V.10 - predict_cost_LASSO_2)^2))
rmse_cost_2
# before Standardizing 39.44415
# after Standardizing 39.70069

# lag3
# Split the lag3 into training and testing datasets.
# change sales to cost
lag3_data <- cbind(lag3_data[,-8], rb_Data[,109])
# rename columns name
colnames(lag3_data)[27] <- c("V.10")
set.seed(12345)
train_indices = sample(1:nrow(lag3_data),0.7*nrow(lag3_data),replace=F)
train_data = lag3_data[train_indices,]
test_data = lag3_data[-train_indices,]

# Use cv.glmnet to do LASSO automatically
x=model.matrix(V.10~ ., train_data)[,-1]       # Get the design matrix. Dummy variables for categorical data are added. The column of ones (corresponding to the intercept term) is removed.
y=train_data$V.10

# Standardizing
xs = scale(x)

cv.out = cv.glmnet(xs,y,alpha=1,nfolds = 10)
bestlam = cv.out$lambda.min
coef = predict(cv.out ,type="coefficients",s=bestlam)
coef

# after Standardizing nothing change
# after LASSO drop V.16,V.21,V.25,V.26 columns
LASSO_cost_model_3 = lm(V.10 ~ V.2+V.3+V.4+V.5+V.6+V.7+V.8+V.11+V.12+V.13+V.14+V.15+V.17+V.18+V.19+V.20+V.22+V.23+V.24+V.27+V.28+V.29, train_data)
predict_cost_LASSO_3 = predict(LASSO_cost_model_3, test_data)
summary(predict_cost_LASSO_3)

rmse_cost_3 = sqrt(mean((test_data$V.10 - predict_cost_LASSO_3)^2))
rmse_cost_3
# 40.52475

# lag4
# Split the lag4 into training and testing datasets.
# change sales to cost
lag4_data <- cbind(lag4_data[,-8], rb_Data[,109])
# rename columns name
colnames(lag4_data)[27] <- c("V.10")
set.seed(12345)
train_indices = sample(1:nrow(lag4_data),0.7*nrow(lag4_data),replace=F)
train_data = lag4_data[train_indices,]
test_data = lag4_data[-train_indices,]

# Use cv.glmnet to do LASSO automatically
x=model.matrix(V.10~ ., train_data)[,-1]       # Get the design matrix. Dummy variables for categorical data are added. The column of ones (corresponding to the intercept term) is removed.
y=train_data$V.10

# Standardizing
xs = scale(x)

cv.out = cv.glmnet(xs,y,alpha=1,nfolds = 10)
bestlam = cv.out$lambda.min
coef = predict(cv.out ,type="coefficients",s=bestlam)
coef

# before Standardizing
# after LASSO drop V.18,V.26 columns
# LASSO_cost_model_4 = lm(V.10 ~ V.2+V.3+V.4+V.5+V.6+V.7+V.8+V.11+V.12+V.13+V.14+V.15+V.16+V.17+V.19+V.20+V.21+V.22+V.23+V.24+V.25+V.27+V.28+V.29, train_data)
# after Standardizing
# after LASSO drop V.26 column
LASSO_cost_model_4 = lm(V.10 ~ V.2+V.3+V.4+V.5+V.6+V.7+V.8+V.11+V.12+V.13+V.14+V.15+V.16+V.17+V.18+V.19+V.20+V.21+V.22+V.23+V.24+V.25+V.27+V.28+V.29, train_data)

predict_cost_LASSO_4 = predict(LASSO_cost_model_4, test_data)
summary(predict_cost_LASSO_4)

rmse_cost_4 = sqrt(mean((test_data$V.10 - predict_cost_LASSO_4)^2))
rmse_cost_4
# before Standardizing 38.79835
# after Standardizing 38.81051

# lag5
# Split the lag5 into training and testing datasets.
# change sales to cost
lag5_data <- cbind(lag5_data[,-8], rb_Data[,109])
# rename columns name
colnames(lag5_data)[27] <- c("V.10")
set.seed(12345)
train_indices = sample(1:nrow(lag5_data),0.7*nrow(lag5_data),replace=F)
train_data = lag5_data[train_indices,]
test_data = lag5_data[-train_indices,]

# Use cv.glmnet to do LASSO automatically
x=model.matrix(V.10~ ., train_data)[,-1]                         # Get the design matrix. Dummy variables for categorical data are added. The column of ones (corresponding to the intercept term) is removed.
y=train_data$V.10

# Standardizing
xs = scale(x)

cv.out = cv.glmnet(xs,y,alpha=1,nfolds = 10)
bestlam = cv.out$lambda.min
coef = predict(cv.out ,type="coefficients",s=bestlam)
coef

# after Standardizing nothing change
# after LASSO drop V.6,V.8,V.12 columns
LASSO_cost_model_5 = lm(V.10 ~ V.2+V.3+V.4+V.5+V.7+V.11+V.13+V.14+V.15+V.16+V.17+V.19+V.20+V.21+V.23+V.24+V.25+V.26+V.27+V.28+V.29, train_data)
predict_cost_LASSO_5 = predict(LASSO_cost_model_5, test_data)
summary(predict_cost_LASSO_5)

rmse_cost_5 = sqrt(mean((test_data$V.10 - predict_cost_LASSO_5)^2))
rmse_cost_5
# 38.7192

# lag_avg
# get lag_avg
# change sales to cost
lag_avg <- cbind(lag_avg[,-8], rb_Data[,109])
# rename columns name
colnames(lag_avg)[27] <- c("V.10")
set.seed(12345)
train_indices = sample(1:nrow(lag_avg),0.7*nrow(lag_avg),replace=F)
train_data = lag_avg[train_indices,]
test_data = lag_avg[-train_indices,]

# Use cv.glmnet to do LASSO automatically
x=model.matrix(V.10~ ., train_data)[,-1]       # Get the design matrix. Dummy variables for categorical data are added. The column of ones (corresponding to the intercept term) is removed.
y=train_data$V.10

# Standardizing
xs = scale(x)

cv.out = cv.glmnet(xs,y,alpha=1,nfolds = 10)
bestlam = cv.out$lambda.min
coef = predict(cv.out ,type="coefficients",s=bestlam)
coef

# before Standardizing
# after LASSO drop V.16,V.25 columns
# LASSO_cost_model = lm(V.10 ~ V.2+V.3+V.4+V.5+V.6+V.7+V.8+V.11+V.12+V.13+V.14+V.15+V.17+V.18+V.19+V.20+V.21+V.22+V.23+V.24+V.26+V.27+V.28+V.29,train_data)
# after Standardizing
# after LASSO drop V.11,V.12,V.15,V.16,V.20,V.21,V.22,V.25,V.26,V.29 columns
LASSO_cost_model = lm(V.10 ~ V.2+V.3+V.4+V.5+V.6+V.7+V.8+V.13+V.14+V.17+V.18+V.19+V.23+V.24+V.27+V.28,train_data)

predict_cost_LASSO = predict(LASSO_cost_model, test_data)
summary(predict_cost_LASSO)

rmse_cost_avg = sqrt(mean((test_data$V.10 - predict_cost_LASSO)^2))
rmse_cost_avg
# before Standardizing 39.62963
# after Standardizing 39.1091

# rmse of sales
rmse_sales_arr <- c(rmse_sales_1,rmse_sales_2,rmse_sales_3,rmse_sales_4,rmse_sales_5,rmse_sales_avg)
rmse_sales_arr
# before Standardizing the predictors 174.5394 168.7033 187.2429 165.7134 165.1048 168.1110
# after Standardizing the predictors  174.5394 168.7033 187.2429 165.0812 163.2143 168.1110

# rmse of cost
rmse_cost_arr <- c(rmse_cost_1,rmse_cost_2,rmse_cost_3,rmse_cost_4,rmse_cost_5,rmse_cost_avg)
rmse_cost_arr
# before Standardizing the predictors 40.18401 39.44415 40.52475 38.79835 38.71920 39.62963
# after  Standardizing the predictors 40.13801 39.70069 40.52475 38.81051 38.71920 39.10910

# 将售价before Standardizing后获得的rmse画直方图对比
sales_rmse_table <- matrix(c(174.5394, 168.7033, 187.2429, 165.7134, 165.1048, 168.1110, 174.5394, 168.7033, 187.2429, 165.0812, 163.2143, 168.1110), nrow=6, ncol=2, dimnames = list(c("lag1", "lag2", "lag3", "lag4", "lag5", "lag6"), c("before", "after")))
sales_rmse_table <- sales_rmse_table |>
  as.table() |>  
  as.data.frame() |>  
  setNames(c("lag", "Standardizing", "sales_rmse")) 
knitr::kable(sales_rmse_table)

ggplot(sales_rmse_table, aes(x = lag, fill = Standardizing, y = sales_rmse)) +
  geom_col(position = "dodge")

min(sales_rmse_table$sales_rmse)

# 将成本before Standardizing后获得的rmse画直方图对比
cost_rmse_table <- matrix(c(40.18401, 39.44415, 40.52475, 38.79835, 38.71920, 39.62963, 40.13801, 39.70069, 40.52475, 38.81051, 38.71920, 39.10910), nrow=6, ncol=2, dimnames = list(c("lag1", "lag2", "lag3", "lag4", "lag5", "lag6"), c("before", "after")))
cost_rmse_table <- cost_rmse_table |>
  as.table() |>  
  as.data.frame() |>  
  setNames(c("lag", "Standardizing", "cost_rmse")) 
knitr::kable(cost_rmse_table)

ggplot(cost_rmse_table, aes(x = lag, fill = Standardizing, y = cost_rmse)) +
  geom_col(position = "dodge")

min(cost_rmse_table$cost_rmse)
